= Kafka-CDI - A simple CDI extension for Apache Kafka

image:https://img.shields.io/travis/matzew/kafka-cdi/master.svg[Build Status (Travis CI), link=https://travis-ci.org/matzew/kafka-cdi]
image:https://img.shields.io/:license-Apache2-blue.svg[License, link=http://www.apache.org/licenses/LICENSE-2.0]
image:https://img.shields.io/maven-central/v/net.wessendorf.kafka/kafka-cdi-extension.svg[Maven Central, link=http://search.maven.org/#search%7Cga%7C1%7Ckafka-cdi-extension]
image:http://www.javadoc.io/badge/net.wessendorf.kafka/kafka-cdi-extension.svg[Javadocs, link=http://www.javadoc.io/doc/net.wessendorf.kafka/kafka-cdi-extension]

Making it easy to use Apache Kafka's Java client API in a CDI managed environment!

== Getting started

This section gives an overview on how to use the CafDI library!

=== Maven config

In a Maven managed project simply the following to your `pom.xml`:

[source,xml]
----
...
  <dependency>
    <groupId>net.wessendorf.kafka</groupId>
    <artifactId>kafka-cdi-extension</artifactId>
    <version>0.0.8</version>
  </dependency>
...
----

=== Injecting a Kafka Producer

The `@Producer` annotation is used to configure and inject an instance of the `SimpleKafkaProducer` class, which is a simple extension of the original `KafkaProducer` class:

[source,java]
----
...
public class MyPublisherService {

  private Logger logger = LoggerFactory.getLogger(MyPublisherService.class);

  @Producer
  SimpleKafkaProducer<Integer, String> producer;

  /**
   * A simple service method, that sends payload over the wire
   */
  public void hello() {
    producer.send("myTopic", "My Message");
  }
}
----

=== Annotating a bean method to declare it as a Kafka Consumer

The `@Consumer` annotation is used to configure and declare an annotated method as a _callback_ for the internal `DelegationKafkaConsumer`, which internally uses the vanilla `KafkaConsumer`:

[source,java]
----
public class MyListenerService {

  private Logger logger = LoggerFactory.getLogger(MyListenerService.class);
  
  /**
   * Simple listener that receives messages from the Kafka broker
    */
  @Consumer(topic = "myTopic", groupId = "myGroupID", keyType = Integer.class)
  public void receiver(String message) {
    logger.info("That's what I got: " + message);
  }
]
----

Receiving the key and the value is also possible:

[source,java]
----
public class MyListenerService {

  private Logger logger = LoggerFactory.getLogger(MyListenerService.class);

  /**
   * Simple listener that receives messages from the Kafka broker
    */
  @Consumer(topic = "myTopic", groupId = "myGroupID")
  public void receiver(final String key, final String value) {
    logger.info("That's what I got: (key: " + key + " , value:" +  value + ")");
  }
]
----



=== Global Configuration of the Kafka cluster

A minimal of configuration is currently needed. For that there is a `@KafkaConfig` annotation. The first occurrence is used:

[source,java]
----
@KafkaConfig(bootstrapServers = "#{SOME_HOST}:#{SOME_PORT}")
public class MyService {
   ...
}
----

=== JsonObject Serialization

Apache Kafka uses a binary message format, and comes with a handful of handy Serializers and Deserializers, available through the `Serdes` class. The CafDI extension adds a Serde for the `JsonObject`:

==== JsonObject Serializer

To send serialize a JsonObject, simply specify the type, like:

[source,java]
----
...
@Producer
SimpleKafkaProducer<Integer, JsonObject> producer;
...

producer.send("myTopic", myJsonObj);
----

==== JsonObject Deserializer

For deserialization the argument on the annotation `@Consumer` method is used to setup the actual `Deserializer`

[source,java]
----
@Consumer(topic = "myTopic", groupId = "myGroupID", keyType = Integer.class)
public void receiveJsonObject(JsonObject message) {
  logger.info("That's what I got: " + message);
}
----

== Running Apache Kafka 

To setup Apache Kafka there are different ways to get started. This section quickly discusses pure Docker and Openshift.

=== Running via Docker images

Starting a Zookeeper cluster:

[source,bash]
----
docker run -d --name zookeeper jplock/zookeeper:3.4.6
----

Next, we need to start Kafka and link the Zookeeper Linux container to it:

[source,bash]
----
docker run -d --name kafka --link zookeeper:zookeeper ches/kafka
----

Now, that the broker is running, we need to figure out the IP address of it:

[source,bash]
----
docker inspect --format '{{ .NetworkSettings.IPAddress }}' kafka  
----

We use this IP address when inside our `@KafkaConfig` annotation that our _Producers_ and _Consumers_ can speak to Apache Kafka.

=== Running on Openshift 

For Apache Kafka on Openshift please check this repository 

https://github.com/EnMasseProject/barnabas
